{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "from heapq import nsmallest, nlargest\n",
    "from typing import List\n",
    "\n",
    "from wordExplorer.utils import getWordData, loadWord2Vec, jsonSimi\n",
    "from wordExplorer.thesa import bighugeCrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words(tx, first: str, sec: List[str], similarity: List[float]):\n",
    "    for second, sim in zip(sec, similarity):\n",
    "        tx.run(\"MERGE (a:Word {word: $first}) \"\n",
    "                \"MERGE (b:Word {word: $second})\"\n",
    "                \"MERGE (a)-[:SYN {sim: $sim}]->(b)\", first=first, second=second, sim=sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCalculator(mName: str = None):\n",
    "    if mName is not None:\n",
    "        model = loadWord2Vec(mName)\n",
    "    else:\n",
    "        model = loadWord2Vec()\n",
    "\n",
    "    driver = GraphDatabase.driver(\"bolt://172.31.128.1:7687/\",\n",
    "                              auth=(\"neo4j\", \"1234\"))\n",
    "\n",
    "    def calcGraph(crawlType, word: str, loc: str = './data/', apiKwargs: dict = None, word2vecKwargs: dict = None) -> nx.Graph:\n",
    "        if apiKwargs is None:\n",
    "            apiKwargs = {}\n",
    "\n",
    "        if word2vecKwargs is None:\n",
    "            word2vecKwargs = {}\n",
    "\n",
    "        #Getting the raw graph structure\n",
    "        try:\n",
    "            with open(loc + f'synonyms/{word}.json', \"r\", encoding=\"utf-8\") as f:\n",
    "                rawdata = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            rawdata = getWordData(crawlType, word, loc + 'synonyms/', **apiKwargs)\n",
    "            # with open(loc + f'synonyms/{word}.json', \"w\", encoding=\"utf-8\") as f:\n",
    "            #     json.dump(data, f)\n",
    "\n",
    "        # G = nx.Graph(data)\n",
    "        \n",
    "        #Getting the weights for the graph\n",
    "        try:\n",
    "            with open(loc + f'weights/{word}.json', \"r\", encoding=\"utf-8\") as f:\n",
    "                weights = jsonSimi(json.load(f))\n",
    "        except FileNotFoundError:\n",
    "            weights, failed = model(rawdata, **word2vecKwargs)\n",
    "            print(f'Total failed: {failed}')\n",
    "            # with open(loc + f'weights/{word}.json', \"w\", encoding=\"utf-8\") as f:\n",
    "            #     json.dump(jsonSimi(weights), f)\n",
    "\n",
    "        dbData = {}\n",
    "\n",
    "        for key, syns in rawdata.items():\n",
    "            validSyn = []\n",
    "            validWeight = []\n",
    "            for syn in syns:\n",
    "                if (key, syn) in weights:\n",
    "                    validSyn.append(syn)\n",
    "                    validWeight.append(weights[(key, syn)])\n",
    "            dbData[key] = [validSyn, validWeight]\n",
    "\n",
    "        with driver.session(database=\"neo4j\") as session:\n",
    "            for key, (synList, synWeight) in dbData.items():\n",
    "                session.execute_write(add_words, key, synList, synWeight)\n",
    "\n",
    "        # remove = [x for x in G.nodes() if x not in weights]\n",
    "        # G.remove_nodes_from(remove)\n",
    "\n",
    "        # nx.set_edge_attributes(G, weights, 'weight')\n",
    "        # return G\n",
    "    \n",
    "    return calcGraph\n",
    "\n",
    "def posTag(char: str) -> str:\n",
    "    punc = \".,!?;“”\"\n",
    "    def puncSwap(char: str, pos: str):\n",
    "        if char in punc:\n",
    "            return 'PUN'\n",
    "        else:\n",
    "            return pos\n",
    "\n",
    "    return list(map(lambda x: (x[0], puncSwap(x[0], x[1])), \\\n",
    "        nltk.pos_tag(nltk.word_tokenize(char))))\n",
    "\n",
    "def lemmatize(char: str) -> str:\n",
    "        lmn = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "        wordnetConv = {\n",
    "            'J': 'a',\n",
    "            'V': 'v',\n",
    "            'N': 'n',\n",
    "            'R': 'r'\n",
    "        }\n",
    "\n",
    "        return map(lambda x: lmn.lemmatize(x[0], wordnetConv.get(x[1][0], 'n')), posTag(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = getCalculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama = [\n",
    "    # 'entangled',\n",
    "    # 'request',\n",
    "    # 'surge',\n",
    "    # 'migrants',\n",
    "    # 'flood',\n",
    "    # 'children',\n",
    "    # 'beefing',\n",
    "    # 'presence',\n",
    "    # 'tougher',\n",
    "    # 'enforcement',\n",
    "    # 'strong',\n",
    "    # 'misgivings',\n",
    "    # 'charged',\n",
    "    # 'shadow',\n",
    "    # 'politics',\n",
    "    # 'emphasize',\n",
    "    # 'trip',\n",
    "    # 'relax',\n",
    "    # 'deportations',\n",
    "    # 'round',\n",
    "    'discussion',\n",
    "    'sweeping',\n",
    "    'actions',\n",
    "    'path',\n",
    "    'bill',\n",
    "    'flexibility',\n",
    "    'treat',\n",
    "    'combating',\n",
    "    'trafficking',\n",
    "    'aggressive',\n",
    "    'plan',\n",
    "    'cut',\n",
    "    'process'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting insertion for: discussion\n",
      "Total failed: 833\n",
      "Attempting insertion for: sweeping\n",
      "Got error code: 404 when trying word defecation\n",
      "Total failed: 7373\n",
      "Attempting insertion for: actions\n",
      "Total failed: 4522\n",
      "Attempting insertion for: path\n",
      "Total failed: 1898\n",
      "Attempting insertion for: bill\n",
      "Total failed: 5497\n",
      "Attempting insertion for: flexibility\n",
      "Total failed: 15\n",
      "Attempting insertion for: treat\n",
      "Total failed: 6017\n",
      "Attempting insertion for: combating\n",
      "Total failed: 1156\n",
      "Attempting insertion for: trafficking\n",
      "Total failed: 658\n",
      "Attempting insertion for: aggressive\n",
      "Total failed: 44\n",
      "Attempting insertion for: plan\n",
      "Got error code: 404 when trying word cause to be perceived\n",
      "Total failed: 1870\n",
      "Attempting insertion for: cut\n",
      "Got error code: 404 when trying word cause to be perceived\n",
      "Got error code: 404 when trying word cock walk\n",
      "Got error code: 404 when trying word defecate\n",
      "Total failed: 20409\n",
      "Attempting insertion for: process\n",
      "Got error code: 500 when trying word 0\n",
      "Got error code: 404 when trying word entity\n",
      "Got error code: 500 when trying word 0\n",
      "Total failed: 6489\n"
     ]
    }
   ],
   "source": [
    "for word in obama:\n",
    "    print(f'Attempting insertion for: {word}')\n",
    "    calc(bighugeCrawl, list(lemmatize(word))[0], './data/crawlers/hugelabs/', apiKwargs={'order': 3, 'waitTime': 0.5})\n",
    "    # print(list(lemmatize(word))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = nx.from_dict_of_dicts(net)\n",
    "g = calc(bighugeCrawl, 'surge', './data/crawlers/hugelabs/', apiKwargs={'order': 2})\n",
    "# h = calc(bighugeCrawl, 'flood', './data/crawlers/hugelabs/', apiKwargs={'order': 3})\n",
    "\n",
    "# g = nx.compose(g, h)\n",
    "# g = nx.intersection(g, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges = nx.get_edge_attributes(g, \"weight\")\n",
    "edges = [g.get_edge_data(u, v).get(\"weight\", 1.0) ** 4 / 12 for u, v in g.edges()]\n",
    "\n",
    "colDict = { idx+1: val for idx, val in enumerate([\n",
    "    '#ff0000',\n",
    "    '#ff4500',\n",
    "    '#ff8900',\n",
    "    '#ffce00',\n",
    "    '#ebff00',\n",
    "    '#a6ff00',\n",
    "    '#62ff00',\n",
    "    '#1dff00',\n",
    "    '#00ff28',\n",
    "    '#00ff6d',\n",
    "    '#00ffb1',\n",
    "    '#00fff6',\n",
    "])}\n",
    "\n",
    "colDict[0] = \"#00FF51\"\n",
    "colDict[-2] = \"#00ffff\"\n",
    "\n",
    "depth = nx.shortest_path_length(g, \"flood\")\n",
    "\n",
    "depth['surge'] = -2\n",
    "\n",
    "nodeCols = [colDict.get(depth.get(key, -1), \"#00ffff\") for key, val in g.nodes().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.kamada_kawai_layout(g)\n",
    "# pos = nx.circular_layout(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(9, 9))\n",
    "# nx.draw(g, pos=pos)\n",
    "nx.draw_networkx_edges(g, pos, width=edges)\n",
    "nx.draw_networkx_nodes(g, pos, node_color=nodeCols, node_size=8)\n",
    "# nx.draw_networkx_labels(g, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "# a, extrema = nx.centrality.eigenvector_centrality(g, max_iter=10**4, weight='weight'), nlargest\n",
    "# a, extrema = nx.centrality.harmonic_centrality(g, distance='weight'), nlargest\n",
    "a, extrema = nx.centrality.closeness_centrality(g, distance='weight'), nlargest\n",
    "\n",
    "ext = extrema(n, a, key=a.get)\n",
    "for key in ext:\n",
    "    print(f'Extrema: {key}, {a[key]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b8270010b70e837fe0de46eb723365a1e31fb6fc612b58e66883d87b37b19d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
